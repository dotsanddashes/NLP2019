{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определение языка и VK API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном задании вам нужно будет:\n",
    "\n",
    "* используя API Вконтакте, скачать комментарии к первым ста постам из пяти сообществ\n",
    "* натренировать модель распознавания языков на статьях из Википедии.\n",
    "* распознать язык всех комментариев, где в тексте есть 10 и более символов, и построить статистику"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VK API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подключения к ВКонтакте мы будем использовать VK API. Здесь есть документация к этой библиотеке https://vk-api.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vk_api\n",
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import vk_api\n",
    "\n",
    "# авторизация\n",
    "vk_session = vk_api.VkApi(login=user, password=password)\n",
    "vk_session.auth()\n",
    "\n",
    "vk = vk_session.get_api() # объект с API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получить записи со страницы можно с помощью метода `wall.get`. Он принимает параметр `domain` — короткое имя пользователя или сообщества — и `count` — количество записей, которое вы хотите получить (максимум — 100). По Список методов для работы со стенами: https://vk.com/dev/wall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, вот так можно получить последние две записи с вот этой страницы https://vk.com/futureisnow. Выдача представляет собой словарь, в котором в поле `items` записан список словарей, содержащий информацию о каждой из записи. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vk.wall.get(domain=\"futureisnow\", count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью метода `groups.getById` можно получить информацию о сообществе, в том числе его id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vk.groups.getById(group_ids=\"futureisnow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачайте информацию о последних ста записях в следующих пабликах: https://vk.com/futureisnow, https://vk.com/eternalclassic, https://vk.com/ukrlit_memes, https://vk.com/ukrainer_net, https://vk.com/amanzohel, https://vk.com/barg_kurumk_culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# список domain'ов, чтобы вам не копировать их самими :)\n",
    "publics = [\"futureisnow\",\n",
    "           \"eternalclassic\",\n",
    "           \"ukrlit_memes\",\n",
    "           \"ukrainer_net\",\n",
    "           \"amanzohel\",\n",
    "           \"barg_kurumk_culture\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = {} # ключи — это паблики\n",
    "for i in publics:\n",
    "    items[i] = {}\n",
    "for k in items.keys():\n",
    "    items[k] = vk.wall.get(domain = k, count = 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите в документации (https://vk.com/dev/wall) метод для получения комментариев и получите первые сто комментариев каждого поста из выборки для каждого паблика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = {}\n",
    "for k in items.keys():\n",
    "    i = 0\n",
    "    while i < 99:\n",
    "        key = items[k]['items'][i]['id']\n",
    "        public_id = items[k]['items'][i]['owner_id']\n",
    "        corpora[key] = vk.wall.getComments(owner_id = public_id, post_id = key, count = 100)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание со звездочкой:** вы могли заметить, что если обращаться к каждому посту отдельно, то все занимает довольно продолжительное время (около пяти минут). Найдите в документации vk_api способ сделать это быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = {}\n",
    "\n",
    "# ваш улучшенный код"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тренировка моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В наших комментариях встречались русский, украинский, английский и бурятский."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = {'ru', 'uk', 'en', 'bxr'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачайте документы, на которых вы будете обучать свои модели. Для наших целей хорошо иметь для каждого языка корпус размером около 50 статей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import wikipedia\n",
    "\n",
    "def get_texts_for_lang(lang, n=10):\n",
    "    wiki_content = []\n",
    "    wikipedia.set_lang(lang)\n",
    "    pages = wikipedia.random(n)\n",
    "    for page_name in pages:\n",
    "        try:\n",
    "            page = wikipedia.page(page_name)\n",
    "            wiki_content.append(\"%s\\n%s\" % (page.title, page.content.replace('=', '')))\n",
    "        except wikipedia.exceptions.WikipediaException:\n",
    "            print(\"Skip %s\" % page_name)\n",
    "    return wiki_content\n",
    "\n",
    "wiki_texts = {}\n",
    "for lang in langs:\n",
    "    wiki_texts[lang] = get_texts_for_lang(lang, 50)\n",
    "    print(lang, len(wiki_texts[lang]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделайте определялку на частотах слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "\n",
    "def collect_freqlist(wiki_pages, max_len=100):\n",
    "    freqlist = Counter()\n",
    "    for text in wiki_pages:\n",
    "        for word in nltk.word_tokenize(text.lower()):\n",
    "            if word.isalpha():\n",
    "                freqlist[word] += 1\n",
    "    return dict(freqlist.most_common(max_len))\n",
    "\n",
    "freq_lists = {}\n",
    "for lang in langs:\n",
    "    freq_lists[lang] = collect_freqlist(wiki_texts[lang])\n",
    "    \n",
    "def simple_lang_detect(freq_lists, text):\n",
    "    counts = Counter()\n",
    "    for lang, freq_list in freq_lists.items():\n",
    "        freq_list = Counter(freq_list)\n",
    "        for word in nltk.word_tokenize(text):\n",
    "            counts[lang] += int(freq_list[word] > 0)\n",
    "    return counts.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделайте определялку на символьных энграммах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction\n",
    "from sklearn import pipeline\n",
    "from sklearn import naive_bayes\n",
    "import numpy as np\n",
    "\n",
    "clf = pipeline.Pipeline([\n",
    "    ('vctr', feature_extraction.text.TfidfVectorizer(ngram_range=(1, 2), analyzer='char')),\n",
    "    ('clf', naive_bayes.MultinomialNB())\n",
    "])\n",
    "\n",
    "\n",
    "all_texts = []\n",
    "lang_indices = []\n",
    "for lang in wiki_texts:\n",
    "    all_texts.extend(wiki_texts[lang])\n",
    "    lang_indices.extend([lang]*len(wiki_texts[lang]))\n",
    "    \n",
    "clf.fit(np.array(all_texts), np.array(lang_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определение языка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определите язык каждого комментария в каждом паблике с помощью определялки на частотах слов и покажите доли языков среди комментариев для каждого паблика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_detects_freqs = {}\n",
    "\n",
    "for k in corpora.keys():\n",
    "    i = 0\n",
    "    if len(corpora[k]['items']) >= 1:\n",
    "        while i < len(corpora[k]['items']):\n",
    "            if 'text' in corpora[k]['items'][i].keys() and len(corpora[k]['items'][i]['text']) >= 10:\n",
    "                key = corpora[k]['items'][i]['id']\n",
    "                lang_detects_freqs[key] = corpora[k]['items'][i]['text']\n",
    "            i += 1\n",
    "\n",
    "for v in lang_detects_freqs.values():\n",
    "    text = str(v)\n",
    "    language = simple_lang_detect(freq_lists, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделайте то же самое для определителя на символьных энграммах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_texts = list(lang_detects_freqs.values())\n",
    "clf.predict(ng_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обсудите работу каждого из классификаторов, обсудите ошибки, объясните разницу в результатах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Сюда вставьте свой текст (эту ячейку можно отредактировать нажав на нее дважды левой кнопкой мышки и сохранить нажав Ctrl+Enter)>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
